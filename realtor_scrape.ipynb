{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uncomment to install packages\n",
    "#conda install -c conda-forge selenium\n",
    "#!pip install fake_useragent\n",
    "#!pip install eventlet\n",
    "#!pip install python-settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time, os\n",
    "import re\n",
    "import random\n",
    "from random import randint\n",
    "import random\n",
    "from time import sleep\n",
    "\n",
    "import pandas as pd\n",
    "from pymongo import MongoClient\n",
    "from pprint import pprint\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "chromedriver = \"/Applications/chromedriver\" # path to the chromedriver executable\n",
    "os.environ[\"webdriver.chrome.driver\"] = chromedriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fake_useragent import UserAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper files\n",
    "import get_proxies as proxies\n",
    "import extractors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Mongo collection for urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = client['philadelphia_home_prices']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['prices_messy', 'listing_urls']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.list_collection_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uncomment to delete the collection\n",
    "#db.drop_collection('listing_urls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['prices_messy', 'listing_data', 'listing_urls']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.list_collection_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "CollectionInvalid",
     "evalue": "collection listing_urls already exists",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCollectionInvalid\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-f89e26be8af4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_collection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'listing_urls'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pymongo/database.py\u001b[0m in \u001b[0;36mcreate_collection\u001b[0;34m(self, name, codec_options, read_preference, write_concern, read_concern, session, **kwargs)\u001b[0m\n\u001b[1;32m    412\u001b[0m                     name in self.list_collection_names(\n\u001b[1;32m    413\u001b[0m                         filter={\"name\": name}, session=s)):\n\u001b[0;32m--> 414\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mCollectionInvalid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"collection %s already exists\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m             return Collection(self, name, True, codec_options,\n",
      "\u001b[0;31mCollectionInvalid\u001b[0m: collection listing_urls already exists"
     ]
    }
   ],
   "source": [
    "col = db.create_collection('listing_urls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.list_collection_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scrape pages for listing urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.realtor.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 44 listings per page. Choose start and stop page\n",
    "page_from = 26\n",
    "page_to = 26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = db.get_collection('listing_urls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(page_from, page_to+1):\n",
    "    #print(i) #prints page that is open in case bot is detected\n",
    "    driver = webdriver.Chrome(chromedriver)\n",
    "    driver.get(url + 'soldhomeprices/Philadelphia_PA/pg-' + str(i))\n",
    "    \n",
    "    sleep(random.uniform(60,120))\n",
    "    \n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    page = str(soup.find_all('div', class_='photo-wrap'))\n",
    "    extensions = re.findall(r'[A-z]+-[A-z]+\\/[0-9-A-z]+[^\"]', page)\n",
    "    \n",
    "    if len(extensions) == 0:\n",
    "        print('bot is caught on page {}'.format(i))\n",
    "        break\n",
    "        \n",
    "    for i in extensions:\n",
    "        page_url = url + i\n",
    "        page = {'url': page_url}\n",
    "        col.insert_one(page)\n",
    "        \n",
    "    sleep(random.uniform(1,3))\n",
    "    \n",
    "    driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chrismalasics/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: count is deprecated. Use estimated_document_count or count_documents instead. Please note that $where must be replaced by $expr, $near must be replaced by $geoWithin with $center, and $nearSphere must be replaced by $geoWithin with $centerSphere\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull urls from mongoDB into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pull urls from mongo db\n",
    "cursor = db.listing_urls.find({})\n",
    "listings = list(cursor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert dictionary values to list\n",
    "listing_urls_list = []\n",
    "for i in listings:\n",
    "    url = i['url']\n",
    "    listing_urls_list.append(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#see first 5 in list\n",
    "listing_urls_list[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get data from urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### new collection for listing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uncomment the code you need to use\n",
    "db.create_collection('listing_data')\n",
    "col = db.get_collection('listing_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.collection_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nIndexesWas': 1, 'ns': 'philadelphia_home_prices.listing_data', 'ok': 1.0}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#uncomment to drop\n",
    "#db.drop_collection('listing_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.collection_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rotate proxies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get list of proxies from free-proxy.net\n",
    "proxies_address = proxies.proxy_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listing_from = 13\n",
    "listing_to = len(listing_urls_list)\n",
    "count = 0\n",
    "tested_proxies = []\n",
    "bad_proxies = []\n",
    "bad_url = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import concurrent.futures\n",
    "from datetime import datetime\n",
    "import eventlet\n",
    "import settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = eventlet.GreenPool(settings.max_threads)\n",
    "pile = eventlet.GreenPile(pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proxies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(listing_from, listing_to+1): \n",
    "    print(i, listing_urls_list[i])\n",
    "\n",
    "    try:\n",
    "        #PROXY = proxies[count].get_address()\n",
    "        PROXY = proxies_address[count]\n",
    "        webdriver.DesiredCapabilities.CHROME['proxy']={\n",
    "                                                        \"httpProxy\":PROXY,\n",
    "                                                       \"ftpProxy\":PROXY,\n",
    "                                                       \"sslProxy\":PROXY,\n",
    "                                                       \"proxyType\":\"MANUAL\",\n",
    "                                                        }\n",
    "        driver = webdriver.Chrome(chromedriver)\n",
    "        driver.get(listing_urls_list[i])\n",
    "\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        listing_data = get_data(soup)\n",
    "        good_proxies.append(PROXY)\n",
    "\n",
    "    except:\n",
    "        bad_proxies.append(PROXY)\n",
    "        count += 1\n",
    "        #driver.close()\n",
    "        \n",
    "    driver.close()\n",
    "    \n",
    "    #insert listing data into mongo database\n",
    "    col.insert_one(listing_data)\n",
    "    print(listing_data)\n",
    "    count += 1\n",
    "    print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threads = min(MAX_THREADS, len(story_urls))\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=threads) as executor:\n",
    "    executor.map(download_url, story_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from extractors import get_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(listing_from, listing_to+1): \n",
    "    print(i, listing_urls_list[i])\n",
    "    check = False\n",
    "    state = False\n",
    "    while state==False:\n",
    "        try:\n",
    "            #PROXY = proxies[count].get_address()\n",
    "            PROXY = proxies_address[count]\n",
    "            print(PROXY)\n",
    "            webdriver.DesiredCapabilities.CHROME['proxy']={\n",
    "                                                            \"httpProxy\":PROXY,\n",
    "                                                           \"ftpProxy\":PROXY,\n",
    "                                                           \"sslProxy\":PROXY,\n",
    "                                                           \"proxyType\":\"MANUAL\",\n",
    "                                                            }\n",
    "            driver = webdriver.Chrome(chromedriver)\n",
    "            driver.get(listing_urls_list[i])\n",
    "\n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "            listing_data = get_data(soup)\n",
    "            \n",
    "            #try:\n",
    "            #    check = (soup.find('span', itemprop=\"streetAddress\")).text\n",
    "            #except:\n",
    "            #    bad_url.append(listing_urls_list[i])\n",
    "            #    break\n",
    "      \n",
    "                \n",
    "            \n",
    "            tested_proxies.append(PROXY)\n",
    "            state = True\n",
    "        except:\n",
    "            bad_proxies.append(PROXY)\n",
    "            count += 1\n",
    "            driver.close()\n",
    "        \n",
    "    driver.close()\n",
    "    \n",
    "    #insert listing data into mongo database\n",
    "    col.insert_one(listing_data)\n",
    "    print(listing_data)\n",
    "    count += 1\n",
    "    print(count)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    #if i//5 == 0:\n",
    "    #    sleep(random.uniform(60,120))\n",
    "        \n",
    "    ####     \n",
    "    #session=requests.Session()\n",
    "    #session.headers = user_agent#{'User-Agent': ua.random} #'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/47.0.2526.106 Safari/537.36'}\n",
    "    #print(user_agent)\n",
    "    ##base_url='https://www.realtor.com'\n",
    "    ##re_list=[]\n",
    "    ##realtor_default_count=44\n",
    "    ##page_counter=1\n",
    "    ##search_url=''.join([base_url, '/realestateandhomes-search/Sacramento_CA'])\n",
    "    #page=session.get(listing_urls_list[i])\n",
    "    #c=page.content\n",
    "    #soup=BeautifulSoup(c,\"html.parser\")\n",
    "    ####\n",
    "    \n",
    "    #options = Options()\n",
    "    #ua = UserAgent()\n",
    "    #userAgent = ua.random\n",
    "    #print(userAgent)\n",
    "    #options.add_argument(f'user-agent={userAgent}')\n",
    "    #driver = webdriver.Chrome(chrome_options=options, executable_path=chromedriver)\n",
    "    #driver.get(listing_urls_list[i])\n",
    "    \n",
    "    #sleep(random.uniform(120,180)) #pause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tested_proxies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_data(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tested_proxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hello = {'hi':[]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hello['hi'] == []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_data(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_proxies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(listing_from, listing_to+1): \n",
    "    \n",
    "    print(i, listing_urls_list[i])\n",
    "    \n",
    "    #ua = UserAgent()\n",
    "    #user_agent = {'User-agent': ua.random}\n",
    "    #print(user_agent)\n",
    "\n",
    "    #response  = requests.get(listing_urls_list[i], headers = user_agent)\n",
    "    \n",
    "    #driver = webdriver.Chrome(chromedriver)\n",
    "    #driver.get(listing_urls_list[i])\n",
    "    \n",
    "    #if i//5 == 0:\n",
    "    #    sleep(random.uniform(60,120))\n",
    "        \n",
    "    ####     \n",
    "    #session=requests.Session()\n",
    "    #session.headers = user_agent#{'User-Agent': ua.random} #'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/47.0.2526.106 Safari/537.36'}\n",
    "    #print(user_agent)\n",
    "    ##base_url='https://www.realtor.com'\n",
    "    ##re_list=[]\n",
    "    ##realtor_default_count=44\n",
    "    ##page_counter=1\n",
    "    ##search_url=''.join([base_url, '/realestateandhomes-search/Sacramento_CA'])\n",
    "    #page=session.get(listing_urls_list[i])\n",
    "    #c=page.content\n",
    "    #soup=BeautifulSoup(c,\"html.parser\")\n",
    "    ####\n",
    "    \n",
    "    #options = Options()\n",
    "    #ua = UserAgent()\n",
    "    #userAgent = ua.random\n",
    "    #print(userAgent)\n",
    "    #options.add_argument(f'user-agent={userAgent}')\n",
    "    #driver = webdriver.Chrome(chrome_options=options, executable_path=chromedriver)\n",
    "    #driver.get(listing_urls_list[i])\n",
    "\n",
    "    ###\n",
    "    \n",
    "    '''options = Options()\n",
    "    ua = UserAgent()\n",
    "    userAgent = ua.random\n",
    "    #print(userAgent)\n",
    "    options.add_argument(f'user-agent={userAgent}')\n",
    "    driver = webdriver.Chrome(chrome_options=options, executable_path=chromedriver)\n",
    "    driver.get(listing_urls_list[i])\n",
    "    #user_agent = {'User-agent': 'Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)'}\n",
    "    #response  = requests.get(listing_urls_list[i], headers = user_agent)\n",
    "    \n",
    "    sleep(random.uniform(60,100))'''\n",
    "    ###\n",
    "    \n",
    "   \n",
    "    \n",
    "    PROXY = proxies[count].get_address()\n",
    "    \n",
    "    \n",
    "    print(PROXY)\n",
    "    webdriver.DesiredCapabilities.CHROME['proxy']={\n",
    "                                                    \"httpProxy\":PROXY,\n",
    "                                                    \"ftpProxy\":PROXY,\n",
    "                                                    \"sslProxy\":PROXY,\n",
    "\n",
    "                                                    \"proxyType\":\"MANUAL\",\n",
    "\n",
    "                                                    }\n",
    "    driver = webdriver.Chrome(chromedriver)\n",
    "    driver.get(listing_urls_list[i])\n",
    "    \n",
    "    \n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    \n",
    "\n",
    "    driver.close()\n",
    "    \n",
    "    #insert listing data into mongo database\n",
    "    col.insert_one(listing_data)\n",
    "    print(listing_data)\n",
    "    count += 1\n",
    "    \n",
    "    #sleep(random.uniform(120,180)) #pause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proxies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listing_data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import requests\n",
    "import random\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "def get_free_proxies():\n",
    "    url = \"https://free-proxy-list.net/\"\n",
    "    # get the HTTP response and construct soup object\n",
    "    soup = bs(requests.get(url).content, \"html.parser\")\n",
    "    proxies = []\n",
    "    for row in soup.find(\"table\", attrs={\"id\": \"proxylisttable\"}).find_all(\"tr\")[1:]:\n",
    "        tds = row.find_all(\"td\")\n",
    "        try:\n",
    "            ip = tds[0].text.strip()\n",
    "            port = tds[1].text.strip()\n",
    "            host = f\"{ip}:{port}\"\n",
    "            proxies.append(host)\n",
    "        except IndexError:\n",
    "            continue\n",
    "    return proxies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = ('hi',50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(num):\n",
    "    return num+12, num-12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plus, minus = test(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proxies[1].get_address()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proxies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROXY = proxies[0].get_address()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROXY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "webdriver.DesiredCapabilities.CHROME['proxy']={\n",
    "                                                            \"httpProxy\":PROXY,\n",
    "                                                           \"ftpProxy\":PROXY,\n",
    "                                                           \"sslProxy\":PROXY,\n",
    "                                                           \"proxyType\":\"MANUAL\",\n",
    "                                                            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(chromedriver)\n",
    "driver.get(listing_urls_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listing_urls_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(listing_urls_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listing_urls_list[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    check = (soup.find('span', itemprop=\"streetAddress\")).text\n",
    "except:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proxies_address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "street_address = ('Not Found')\n",
    "test = {'street_address': street_address}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listing_data['street_address'] == 'Not Found'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.list_database_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = client.get_database('philadelphia_home_prices')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = db.get_collection('listing_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor = db.listing_data.find({})\n",
    "listings = list(cursor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(listings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import get_proxies as proxies\n",
    "from extractors import get_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(chromedriver)\n",
    "driver.get(\"https://www.realtor.com/realestateandhomes-detail/4506-Locust-St_Philadelphia_PA_19139_M33501-78957\")\n",
    "\n",
    "soup = BeautifulSoup(driver.page_source, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listing_data = get_data(soup)\n",
    "print(listing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup_string = str(soup)\n",
    "soup_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
